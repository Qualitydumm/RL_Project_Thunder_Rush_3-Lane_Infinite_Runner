# âš¡Thunder Rush: 3-Lane Infinite Runner RL Agentâš¡

Cloud RushëŠ” **3-lane ë¬´í•œ ë‹¬ë¦¬ê¸° ê²Œì„ í™˜ê²½**ì—ì„œ  
Agentê°€ **ë‘ ê°œì˜ ê°•í™”í•™ìŠµ(DQN, PPO) ì•Œê³ ë¦¬ì¦˜** ìœ¼ë¡œ ì¥ì• ë¬¼ì„ íšŒí”¼í•˜ë©° ìµœëŒ€í•œ ì˜¤ë˜ ìƒì¡´í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

í™˜ê²½ ì„¤ê³„ë¶€í„° RL ì•Œê³ ë¦¬ì¦˜(DQN, PPO) ë¹„êµÂ·ê°œì„ ê¹Œì§€ ì§ì ‘ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.

---

## ğŸ¯ í”„ë¡œì íŠ¸ ì†Œê°œ (Project Overview)

- **ëª©í‘œ:** Agentê°€ ì¥ì• ë¬¼ì„ íšŒí”¼í•˜ë©° ìµœì¥ ìƒì¡´ ì‹œê°„ ë‹¬ì„±
- **í™˜ê²½:** 3-Lane Infinite Runner (custom Gym-like environment)
- **ì•Œê³ ë¦¬ì¦˜:** Double+â€‹Dueling DQN, PPO+GAE
- **í•™ìŠµ ë°©ì‹:** Vectorized Parallel Training  
  - DQN: 256 environments  
  - PPO: 64 environments
    
---

## ğŸ® Environment Overview 

- **í–‰ë™(Action)** : discrete space(5)
   - 0: ìœ ì§€ (stay)
   - 1: ì™¼ìª½ ì´ë™ (left)
   - 2: ì˜¤ë¥¸ìª½ ì´ë™ (right) 
   - 3: ì í”„ (jump)
   - 4: ìŠ¬ë¼ì´ë“œ (slide)
    
- **ì¥ì• ë¬¼ íƒ€ì…(Obstacles)** 
  - **A**: ì í”„ë¡œë§Œ íšŒí”¼ ê°€ëŠ¥
  - **B**: ìŠ¬ë¼ì´ë“œë¡œë§Œ íšŒí”¼ ê°€ëŠ¥
  - **C**: í”¼í•  ìˆ˜ ì—†ëŠ” íŒ¨í„´ (unavoidable) / ì¢Œìš°ì´ë™ìœ¼ë¡œë§Œ íšŒí”¼ ê°€ëŠ¥

- **ìƒíƒœ(State) ì˜ˆì‹œ**
```python
  [player_lane, speed, time_ratio,
   lane0_exists, lane0_dist, lane0_type,
   lane1_exists, lane1_dist, lane1_type,
   lane2_exists, lane2_dist, lane2_type, ...] 
```

- **ë³´ìƒ(Reward)**
  - ê¸°ë³¸ ìƒì¡´ ë³´ìƒ 
    ë§¤ time step ë§ˆë‹¤ +0.1
  - ì¥ì• ë¬¼ íšŒí”¼ ë³´ìƒ
    ì¥ì• ë¬¼ì´ dist â‰¤ 0ì— ë„ë‹¬í•˜ë©´ ì§€ë‚˜ê°„ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë³´ìƒì„ ë¶€ì—¬
    - A/B ì¥ì• ë¬¼
      ì ì ˆí•œ action: +2.0
      ë‹¤ë¥¸ actionìœ¼ë¡œ ìƒì¡´: +1.0
    - C ì¥ì• ë¬¼
      ì ì ˆí•œ action: +2.0
      í‹€ë¦° action: episode terminate
  - Penalty
    - ì´ë™ penalty : ë¶ˆí•„ìš”í•œ lane ì´ë™ì‹œ, -0.01
    - ì¶©ëŒ penalty : ì¥ì• ë¬¼ í”¼í•˜ì§€ ëª»í• ì‹œ -10.0, episode terminate
			(ì‚¬ë§ì‚¬ìœ ëŠ” í†µê³„ë¡œ ì €ì¥ë¨ :  (A_no_jump, B_no_slide, C_unavoidable))

--- 

## ğŸ§  RL Algorithms

ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” ë‘ ê°€ì§€ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ë¹„êµ/ì‹¤í—˜í•©ë‹ˆë‹¤.

### 1) Double DQN + Dueling Architecture (Vectorized)

- Double DQN:  
  - í–‰ë™ ì„ íƒ: policy_net  
  - Qê°’ í‰ê°€: target_net  
  - max ì—°ì‚°ìœ¼ë¡œ ì¸í•œ Q overestimation ì™„í™”

- Dueling DQN:  
  - Q(s,a) = V(s) + A(s,a) - mean(A(s,Â·))  
  - ìƒíƒœ ê°€ì¹˜ì™€ í–‰ë™ ì´ì ì„ ë¶„ë¦¬í•´ì„œ ë” ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµ
- Vectorized Env:  
  - 256ê°œ í™˜ê²½ ë™ì‹œ ì‹¤í–‰ë¡œ ë§¤ stepë§ˆë‹¤ 256 transition ìˆ˜ì§‘

### 2) PPO + GAE (Proximal Policy Optimization)

- on-policy actorâ€“critic ì•Œê³ ë¦¬ì¦˜
- GAE(Î»)ë¥¼ ì‚¬ìš©í•´ biasâ€“variance trade-off ì¡°ì ˆ
- Clipped objectiveë¡œ ì •ì±… ì—…ë°ì´íŠ¸ í­ì„ ì œí•œí•´ ì•ˆì •ì  í•™ìŠµ
- 64ê°œ í™˜ê²½ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ rollout ê¸°ë°˜ í•™ìŠµ

---

## ğŸ¥ ì‹œê° ìë£Œ 

> ê²Œì„ í”Œë ˆì´ ì´ë¯¸ì§€, í•™ìŠµ ê³¡ì„ , ì‚¬ë§ ë¶„í¬ ë“± ë„£ì„ ì˜ˆì •

| ê²Œì„ í™”ë©´ | í•™ìŠµ ë³´ìƒ ê³¡ì„  |
|----------|----------------|
| ![game](assets/gameplay.gif) | ![reward](assets/reward_curve.png) |

(íŒŒì¼ ì¶”ê°€ í›„ ê²½ë¡œ ë§ì¶° ë„£ìœ¼ë©´ ë¨)

---

## ğŸ›  ì„¤ì¹˜ ë°©ë²• (Installation)

```bash
git clone https://github.com/Qualitydumm/RL_Project_Thunder_Rush_3-Lane_Infinite_Runner.git
cd RL_Project_Thunder_Rush_3-Lane_Infinite_Runner

# (ì„ íƒ) ê°€ìƒí™˜ê²½ ê¶Œì¥
# python -m venv venv
# source venv/bin/activate  # Windows: venv\Scripts\activate

pip install -r requirements.txt
```
---

## ğŸ›  ì‚¬ìš©ë²• (Usage)

1) DQN í•™ìŠµ
python train_dqn_vector_seed_2.py

2) PPO í•™ìŠµ
python train_ppo_vector_seed_2.py

3) ê²Œì„ ì‹¤í–‰
python subway_env_latency_test.py

4) train ê³¼ì • í†µê³„ ë¶„ì„ ë° csv ì €ì¥
python stats_logger.py

random number generator seed ë³€ê²½ì„ í†µí•´ ì‹¤í—˜ ë° ì‹ ë¢°êµ¬ê°„ì„ ì‘ì„±í•˜ê¸° ìœ„í•´ seed_0, seed_1, seed_2 ëª¨ë‘ ì—…ë¡œë“œí•˜ì˜€ìŠµë‹ˆë‹¤.
ì–´ë–¤ ì½”ë“œë¥¼ ì„ íƒí•´ë„ ì§„í–‰ì—ëŠ” ì§€ì¥ì´ ì—†ìŠµë‹ˆë‹¤.

## ğŸ“„ License
This project is licensed under the MIT License.
